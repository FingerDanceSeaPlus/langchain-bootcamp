{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a83d157",
   "metadata": {},
   "source": [
    "# RAG Agent\n",
    "首先安装必须的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ab9860",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain langchain-text-splitters langchain-community bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08fc311",
   "metadata": {},
   "source": [
    "## LangSmith\n",
    "使用LangChain构建的许多应用程序将包含多个步骤，并调用多个LLM调用。随着这些应用变得越来越复杂，检测究竟什么在你的agent或chain中运行显得格外重要，这时候就要用到 LangSmith。\n",
    "\n",
    "提前配置API密钥之类的操作在这里就不赘述了。\n",
    "## 模型和向量库的导入\n",
    "配置聊天模型、嵌入模型和向量库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "765b1ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "# 配置聊天模型\n",
    "model = ChatOpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "    model=\"qwen-plus\",  # 此处以qwen-plus为例，您可按需更换模型名称。模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n",
    "    # other params...\n",
    ")\n",
    "# 配置嵌入模型\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "embeddings = DashScopeEmbeddings(\n",
    "    model=\"text-embedding-v2\",\n",
    "    # other params...\n",
    ")\n",
    "# 配置向量库\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0946b2",
   "metadata": {},
   "source": [
    "## 1. 索引\n",
    "和语义搜索基本相似。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efd497f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 43047\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "assert len(docs) == 1\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "791ddee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdf5c293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 63 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02d7ec80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f708c48a-741c-434e-934e-77882926be89', '7c83bc5b-1784-42f9-b5e6-8f6895ae00ff', '678673b4-181b-44f0-9659-19a1f2b09aa1']\n"
     ]
    }
   ],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63d7eef",
   "metadata": {},
   "source": [
    "## 召回和生成\n",
    "![](./Imaegs/rag_retrieval_generation.avif)\n",
    "1. 召回：根据用户输入，从知识库中检索出最相关的文档片段。\n",
    "2. 生成：根据召回的文档片段和用户的问题生成prompt，生成用户所需的答案。\n",
    "   \n",
    "编写实际的应用程序逻辑。我们希望创建一个简单的应用程序，它接受用户问题，搜索与该问题相关的文档，将检索到的文档和初始问题传递给模型，并返回答案。\n",
    "\n",
    "可以构建一个装备了检索向量数据库工具的简单RAG agent。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6dfbfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f22ddc",
   "metadata": {},
   "source": [
    "这里我们使用工具装饰器来配置工具，将原始文档作为工件附加到每个ToolMessage。这将允许我们访问应用程序中的文档元数据，而不是发送给模型的字符串化表示。\n",
    "\n",
    "检索工具不限于单个字符串查询参数，如下面的示例所示。你可以通过添加参数来强制LLM指定额外的搜索参数——例如，一个类别："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b40c3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from typing import Literal\n",
    "\n",
    "#def retrieve_context(query: str, section: Literal[\"beginning\", \"middle\", \"end\"]):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dedad4d",
   "metadata": {},
   "source": [
    "构建agent：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01544296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "\n",
    "tools = [retrieve_context]\n",
    "# If desired, specify custom instructions\n",
    "prompt = (\n",
    "    \"You have access to a tool that retrieves context from a blog post. \"\n",
    "    \"Use the tool to help answer user queries.\"\n",
    ")\n",
    "agent = create_agent(model, tools, system_prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397ae66d",
   "metadata": {},
   "source": [
    "下面进行测试，我们构造一个问题，它通常需要一系列迭代的检索步骤来回答："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bb3715a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the standard method for Task Decomposition?\n",
      "\n",
      "Once you get the answer, look up common extensions of that method.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_context (call_1b7e5b659144467a85ba2e)\n",
      " Call ID: call_1b7e5b659144467a85ba2e\n",
      "  Args:\n",
      "    query: standard method for Task Decomposition\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "\n",
      "Source: {'start_index': 2578, 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_context (call_b246fa3bb1094284a2c8c5)\n",
      " Call ID: call_b246fa3bb1094284a2c8c5\n",
      "  Args:\n",
      "    query: common extensions of Chain of Thought method\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 6069}\n",
      "Content: Chain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\{(x, y_i , r_i , z_i)\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\geq r_{n-1} \\geq \\dots \\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\tau_h = (x, z_i, y_i, z_j, y_j, \\dots, z_n, y_n)$, where $\\leq i \\leq j \\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The standard method for Task Decomposition is **Chain of Thought (CoT)** prompting. In this approach, the model is instructed to \"think step by step,\" breaking down complex tasks into smaller, more manageable substeps. This allows the model to use more test-time computation and improves performance on complicated reasoning tasks.\n",
      "\n",
      "A common extension of the Chain of Thought method is the **Tree of Thoughts (ToT)** framework (Yao et al., 2023). ToT extends CoT by exploring multiple reasoning paths at each step. It:\n",
      "- Decomposes the problem into multiple thought steps.\n",
      "- Generates several possible thoughts (or solutions) per step, forming a tree-like structure.\n",
      "- Uses search algorithms like breadth-first search (BFS) or depth-first search (DFS) to navigate the tree.\n",
      "- Evaluates each state using a classifier (via prompting) or majority voting to decide which branches to pursue.\n",
      "\n",
      "Another related extension is **Chain of Hindsight (CoH)** (Liu et al., 2023), which focuses on improving model outputs through iterative refinement based on feedback. While not a direct task decomposition method, it enhances reasoning by allowing the model to reflect on previous outputs and associated human feedback, gradually producing better results.\n",
      "\n",
      "These extensions build upon the foundational idea of CoT by incorporating structured exploration (ToT) or learning from iterative feedback (CoH), enabling more robust and flexible problem-solving in AI agents.\n"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    \"What is the standard method for Task Decomposition?\\n\\n\"\n",
    "    \"Once you get the answer, look up common extensions of that method.\"\n",
    ")\n",
    "\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea26f49e",
   "metadata": {},
   "source": [
    "步骤：\n",
    "\n",
    "1. 生成查询以搜索任务分解的标准方法；\n",
    "2. 接收到答案后，生成第二个查询以搜索其公共扩展；\n",
    "3. 在接受了所有必要的上下文之后，回答了这个问题。\n",
    "\n",
    "## RAG chains\n",
    "上文方法存在一些缺点：\n",
    "1. 两个推理调用——在执行搜索时，需要一个调用生成查询，另一个调用生成最终响应。\n",
    "2. 缺少控制——LLM可能跳过某些实际上需要的搜索或者额外执行本来不需要的搜索。\n",
    "\n",
    "另一种常见的方法是两步链，在这种方法中，我们总是运行搜索（可能使用原始用户查询），并将结果合并为单个LLM查询的上下文。这将导致每个查询只有一个推理调用，以牺牲灵活性为代价来减少延迟。\n",
    "\n",
    "在这种方法中，我们不再在循环中调用模型，而是进行单遍传递。\n",
    "\n",
    "我们可以通过从代理中删除工具，并将检索步骤合并到自定义提示符中来实现这个链："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43eb6ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "\n",
    "@dynamic_prompt\n",
    "def prompt_with_context(request: ModelRequest) -> str:\n",
    "    \"\"\"Inject context into state messages.\"\"\"\n",
    "    last_query = request.state[\"messages\"][-1].text\n",
    "    retrieved_docs = vector_store.similarity_search(last_query)\n",
    "\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "    system_message = (\n",
    "        \"You are a helpful assistant. Use the following context in your response:\"\n",
    "        f\"\\n\\n{docs_content}\"\n",
    "    )\n",
    "\n",
    "    return system_message\n",
    "\n",
    "\n",
    "agent = create_agent(model, tools=[], middleware=[prompt_with_context])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d6e46b",
   "metadata": {},
   "source": [
    "测试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ff92c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is task decomposition?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Task decomposition is the process of breaking down a complex task into smaller, more manageable steps or subtasks. This approach allows for better planning and execution, as each simpler step can be addressed individually, making the overall task easier to handle.\n",
      "\n",
      "In the context of AI and language models, task decomposition can be achieved through methods like:\n",
      "\n",
      "- **Chain of Thought (CoT)**: The model is prompted to \"think step by step,\" generating intermediate reasoning steps that lead to the final solution.\n",
      "- **Tree of Thoughts (ToT)**: An extension of CoT where multiple reasoning paths are explored at each step, forming a tree-like structure of possible solutions, which can be searched using strategies like breadth-first or depth-first search.\n",
      "- **LLM+P**: Uses an external classical planner with Planning Domain Definition Language (PDDL) to generate a formal plan, leveraging structured planning tools beyond the LLM’s internal reasoning.\n",
      "\n",
      "These techniques enhance problem-solving by enabling models to decompose tasks, explore alternatives, and reflect on potential outcomes—improving performance on complex, multi-step problems.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is task decomposition?\"\n",
    "for step in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chapter-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
